"""
Converts a dataset in LJSpeech format into audio tokens that can be used to train/fine-tune Soprano.
This script creates two JSON files for train and test splits in the provided directory.

Usage:
python generate_dataset.py --input-dir path/to/files

Args:
--input-dir: Path to directory of LJSpeech-style dataset. If none is provided this defaults to the provided example dataset.
"""
import argparse
import pathlib
import random
import json

from scipy.io import wavfile
import torchaudio
import torch
from tqdm import tqdm
from huggingface_hub import hf_hub_download

from encoder.codec import Encoder


SAMPLE_RATE = 32000
SEED = 42
VAL_PROP = 0.1
VAL_MAX = 512

def get_args():
    parser = argparse.ArgumentParser()
    parser.add_argument("--input-dir",
        required=False,
        default="./example_dataset",
        type=pathlib.Path
    )
    return parser.parse_args()

def main():
    args = get_args()
    input_dir = args.input_dir

    print("Loading model.")
    encoder = Encoder()
    encoder_path = hf_hub_download(repo_id='ekwek/Soprano-Encoder', filename='encoder.pth')
    encoder.load_state_dict(torch.load(encoder_path))
    encoder.eval() # Good practice to set to eval mode
    print("Model loaded.")

    print("Reading metadata.")
    files = []
    # Ensure metadata.txt exists or handle exception
    try:
        with open(f'{input_dir}/metadata.txt', encoding='utf-8') as f:
            data = f.read().split('\n')
            for line in data:
                if not line: continue
                filename, transcript = line.split('|', maxsplit=1)
                files.append((filename, transcript))
        print(f'{len(files)} samples located in directory.')
    except FileNotFoundError:
        print(f"Error: metadata.txt not found in {input_dir}")
        return

    # Define Mel Spectrogram transform
    # Soprano uses 32kHz audio, and the encoder expects 50 channels (n_mels=50).
    mel_transform = torchaudio.transforms.MelSpectrogram(
        sample_rate=SAMPLE_RATE,
        n_fft=1024,
        win_length=1024,
        hop_length=320,  # 10ms at 32kHz
        n_mels=50,       # Matches the model's input channels
        center=True,
        power=1.0,
    )

    print("Encoding audio.")
    dataset = []
    for sample in tqdm(files):
        filename, transcript = sample
        wav_path = f'{input_dir}/wavs/{filename}.wav'
        
        try:
            audio, sr = torchaudio.load(wav_path)
        except Exception as e:
            print(f"Could not load {wav_path}: {e}")
            continue

        if audio.shape[0] > 1: audio = audio.mean(dim=0, keepdim=True)
        if sr != SAMPLE_RATE:
            audio = torchaudio.functional.resample(audio, sr, SAMPLE_RATE)
        
        # Compute Mel Spectrogram
        # Input audio: [1, Time]
        mels = mel_transform(audio) # Output: [1, 50, Mel_Time]
        
        # Apply Log-Mel scaling (standard for neural encoders)
        mels = torch.log(torch.clamp(mels, min=1e-5))

        # The encoder expects [Batch, Channels, Length].
        # mels is [1, 50, Mel_Time], which works as Batch=1.
        
        with torch.no_grad():
            audio_tokens = encoder(mels)
        
        dataset.append([transcript, audio_tokens.squeeze(0).tolist()])

    print("Generating train/test splits.")
    random.seed(SEED)
    random.shuffle(dataset)
    num_val = min(int(VAL_PROP * len(dataset)) + 1, VAL_MAX)
    train_dataset = dataset[num_val:]
    val_dataset = dataset[:num_val]
    print(f'# train samples: {len(train_dataset)}')
    print(f'# val samples: {len(val_dataset)}')

    print("Saving datasets.")
    with open(f'{input_dir}/train.json', 'w', encoding='utf-8') as f:
        json.dump(train_dataset, f, indent=2)
    with open(f'{input_dir}/val.json', 'w', encoding='utf-8') as f:
        json.dump(val_dataset, f, indent=2)
    print("Datasets saved.")


if __name__ == '__main__':
    main()
